{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5ffabd62",
   "metadata": {},
   "source": [
    "# Clustering over INCOME and AGE across the IRIS"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eede670c",
   "metadata": {},
   "source": [
    "# Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa2bc375",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install openpyxl   # for pd.read_excel\n",
    "%pip install ipywidgets # for tqdm.notebook\n",
    "%pip install pot        # for ot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee013e46",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import ot    # pip install pot\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from tqdm.notebook import tqdm\n",
    "import geopandas as gpd\n",
    "\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "import sys\n",
    "sys.path.append(\"../src\")\n",
    "from clustering_methods import bary_WKMeans, dist_WKMeans\n",
    "from utils import reconstruct_joint_distribution_ot, create_regular_grid, project_distribution_on_grid, computeDistanceMatrix, plot_projected_distribution\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "256a2d6e",
   "metadata": {},
   "source": [
    "### Income Deciles per IRIS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e9db764",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the file\n",
    "df_income_temp = pd.read_csv('../data/BASE_TD_FILO_DEC_IRIS_2020.csv', sep=';', decimal=',')\n",
    "\n",
    "\n",
    "# Decile columns\n",
    "decile_cols = ['DEC_D120', 'DEC_D220', 'DEC_D320', 'DEC_D420', 'DEC_MED20', 'DEC_D620', 'DEC_D720', 'DEC_D820', 'DEC_D920']\n",
    "\n",
    "# Convert from object to float64\n",
    "df_income_temp[decile_cols] = df_income_temp[decile_cols].apply(pd.to_numeric, errors='coerce')\n",
    "\n",
    "# Remove the 1319 rows with missing deciles\n",
    "df_income = df_income_temp.dropna(subset=decile_cols).reset_index(drop=True)\n",
    "\n",
    "distributions = df_income[decile_cols].to_numpy()\n",
    "\n",
    "# Useful \n",
    "colors = ['cornflowerblue', 'forestgreen', 'red', 'deeppink', 'orange', 'brown', 'purple']\n",
    "age_groups = [\"0_17\", \"18_29\", \"30_39\", \"40_49\", \"50_64\", \"65_74\", \"75P\"]\n",
    "\n",
    "\n",
    "n_distributions = distributions.shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62ccd981",
   "metadata": {},
   "source": [
    "### Average Income per age"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d0ab763",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the file\n",
    "df_income_age = pd.read_excel(\"../data/reve-niv-vie-individu-age-med.xlsx\", skiprows=3)\n",
    "\n",
    "# Keep only the 2020 column\n",
    "df_income_age = df_income_age[[\"Tranche d’âge\", '2020³ ⁴']]\n",
    "\n",
    "# Rename the column\n",
    "df_income_age = df_income_age.rename(columns={'2020³ ⁴': \"MEDIAN_INCOME\"})\n",
    "\n",
    "# Rename the age groups\n",
    "renaming_dict = {\n",
    "    \"Moins de 18 ans\": \"0_17\",\n",
    "    \"De 18 à 29 ans\": \"18_29\",\n",
    "    \"De 30 à 39 ans\": \"30_39\",\n",
    "    \"De 40 à 49 ans\": \"40_49\",\n",
    "    \"De 50 à 64 ans\": \"50_64\",\n",
    "    \"De 65 à 74 ans\": \"65_74\",\n",
    "    \"75 ans et plus\": \"75P\"\n",
    "}\n",
    "df_income_age[\"AGE_GROUP\"] = df_income_age[\"Tranche d’âge\"].replace(renaming_dict)\n",
    "\n",
    "# Get rid of the metadata lines\n",
    "df_income_age = df_income_age[df_income_age['AGE_GROUP'].isin(age_groups)].reset_index(drop=True)\n",
    "\n",
    "# Keep only the relevant columns\n",
    "df_income_age = df_income_age[[\"AGE_GROUP\", \"MEDIAN_INCOME\"]]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cbf052b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_income.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0674967",
   "metadata": {},
   "source": [
    "### Population per age "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2133701",
   "metadata": {},
   "source": [
    "To apply our algorithms using income by age group, we need to estimate the population in each corresponding age range for every IRIS. However, the INSEE population dataset provides age groupings that do not match exactly with those used in the national income-by-age statistics. To reconcile the two, we reconstruct the target age groups — (0–17), (18–29), (30–39), (40–49), (50–64), (65–74), and 75+ — by combining and subtracting available groups from the population dataset. In some cases, we rely on approximations (e.g., estimating the 45–54 group to split 40–49 and 50–64) to ensure consistency with the income data structure. This harmonization step is essential to later compute joint distributions of age and income at the IRIS level."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8373101",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the Excel sheet\n",
    "df_pop_temp = pd.read_excel(\"../data/base-ic-evol-struct-pop-2020.xlsx\", sheet_name=0, skiprows=5)\n",
    "\n",
    "# Directly accessible age groups\n",
    "df_pop_temp[\"AGE_0_17\"] = df_pop_temp[\"P20_POP0002\"] + df_pop_temp[\"P20_POP0305\"] + df_pop_temp[\"P20_POP0610\"] + df_pop_temp[\"P20_POP1117\"]\n",
    "df_pop_temp[\"AGE_18_29\"] = df_pop_temp[\"P20_POP0014\"] + df_pop_temp[\"P20_POP1529\"] - df_pop_temp[\"AGE_0_17\"]\n",
    "df_pop_temp[\"AGE_30_39\"] = df_pop_temp[\"P20_POP2539\"] + df_pop_temp[\"P20_POP1824\"] - df_pop_temp[\"AGE_18_29\"]\n",
    "df_pop_temp[\"AGE_65_74\"] = df_pop_temp[\"P20_POP65P\"] - df_pop_temp[\"P20_POP75P\"]\n",
    "# No processing needed for 75+ age group\n",
    "\n",
    "# Approximation of the 45–54 group to help deduce 40–49 and 50–64\n",
    "df_pop_temp[\"AGE_45_54\"] = (\n",
    "    df_pop_temp[\"AGE_0_17\"] + df_pop_temp[\"P20_POP1824\"] + df_pop_temp[\"P20_POP2539\"] + df_pop_temp[\"P20_POP4054\"]\n",
    "    - (df_pop_temp[\"P20_POP0014\"] + df_pop_temp[\"P20_POP1529\"] + df_pop_temp[\"P20_POP3044\"])\n",
    ")\n",
    "\n",
    "# Approximate 40–49 from 40–54 and part of 45–54\n",
    "df_pop_temp[\"AGE_40_49\"] = df_pop_temp[\"P20_POP4054\"] - 0.5 * df_pop_temp[\"AGE_45_54\"]\n",
    "\n",
    "# Approximate 50–64 from 55–64 and part of 45–54\n",
    "df_pop_temp[\"AGE_50_64\"] = df_pop_temp[\"P20_POP5564\"] + 0.5 * df_pop_temp[\"AGE_45_54\"]\n",
    "\n",
    "# Renaming for clarity\n",
    "df_pop_temp[\"0_17\"] = df_pop_temp[\"AGE_0_17\"]\n",
    "df_pop_temp[\"18_29\"] = df_pop_temp[\"AGE_18_29\"]\n",
    "df_pop_temp[\"30_39\"] = df_pop_temp[\"AGE_30_39\"]\n",
    "df_pop_temp[\"40_49\"] = df_pop_temp[\"AGE_40_49\"]\n",
    "df_pop_temp[\"50_64\"] = df_pop_temp[\"AGE_50_64\"]\n",
    "df_pop_temp[\"65_74\"] = df_pop_temp[\"AGE_65_74\"]\n",
    "df_pop_temp[\"75P\"] = df_pop_temp[\"P20_POP75P\"]\n",
    "\n",
    "# Final columns to keep\n",
    "final_cols = [\"IRIS\", \"REG\", \"DEP\", \"LIBCOM\", \"LIBIRIS\"] + age_groups\n",
    "\n",
    "df_pop = df_pop_temp[final_cols]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6136029b",
   "metadata": {},
   "source": [
    "## Joint Age-Income Distributions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4407567c",
   "metadata": {},
   "source": [
    "### Reconstructing the joint age–income distribution"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97469093",
   "metadata": {},
   "source": [
    "For each IRIS, we reconstruct a joint probability distribution π over age groups and income levels. The marginal distributions are:\n",
    "* the age distribution of the IRIS (based on population counts per age group), and\n",
    "* a uniform distribution over the income deciles observed in the IRIS.\n",
    "\n",
    "We use optimal transport to find the coupling π that best aligns the income levels with the national average income for each age group, minimizing the cost:\n",
    "\n",
    "$$\n",
    "C_{i,j} = |R_j - m(T_i)|\n",
    "$$\n",
    "\n",
    "where $R_j$ is the j-th income level (decile), and $m(T_i)$ is the national average income for age group $T_i$.\n",
    "\n",
    "The result is a joint distribution π (a matrix), interpreted as the most plausible alignment between age and income in the IRIS, under minimal deviation from national trends."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15c1e39e",
   "metadata": {},
   "source": [
    "### Computation for Each IRIS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bcd8f52",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute national median income per age group\n",
    "age_income_medians = df_income_age.set_index(\"AGE_GROUP\").loc[age_groups, \"MEDIAN_INCOME\"].to_numpy()\n",
    "\n",
    "# Find common IRIS identifiers present in both datasets\n",
    "common_iris = sorted(set(df_income[\"IRIS\"]).intersection(df_pop[\"IRIS\"]))\n",
    "\n",
    "# Compute joint distributions\n",
    "joint_distributions = []\n",
    "joint_supports = []\n",
    "iris_ids = []\n",
    "\n",
    "for iris in tqdm(common_iris):\n",
    "    row_income = df_income[df_income[\"IRIS\"] == iris]\n",
    "    row_pop = df_pop[df_pop[\"IRIS\"] == iris]\n",
    "\n",
    "    if row_income.empty or row_pop.empty:\n",
    "        continue  # Skip if data is missing for this IRIS\n",
    "\n",
    "    # Age group weights (normalized)\n",
    "    age_counts = row_pop.iloc[0][age_groups].to_numpy()\n",
    "    age_weights = age_counts / age_counts.sum()\n",
    "\n",
    "    # Income decile values\n",
    "    income_deciles = row_income.iloc[0][decile_cols].to_numpy()\n",
    "\n",
    "    # Compute the joint distribution π using optimal transport\n",
    "    pi, supp= reconstruct_joint_distribution_ot(age_weights, income_deciles, age_income_medians)\n",
    "\n",
    "    joint_supports.append(supp)\n",
    "    joint_distributions.append(pi)\n",
    "    iris_ids.append(iris)\n",
    "\n",
    "# Normalisation des supports pour équilibrer les échelles âge/revenu\n",
    "def normalize_supports(supports):\n",
    "    all_supports = np.vstack(supports)\n",
    "    min_vals = all_supports.min(axis=0)\n",
    "    max_vals = all_supports.max(axis=0)\n",
    "    normalized = [(s - min_vals) / (max_vals - min_vals) for s in supports]\n",
    "    return normalized, min_vals, max_vals\n",
    "\n",
    "joint_supports, support_min, support_max = normalize_supports(joint_supports)\n",
    "\n",
    "\n",
    "# Stack the joint distributions into a 3D array: (n_iris, n_age_groups, n_income_deciles)\n",
    "joint_supports = np.stack(joint_supports)\n",
    "joint_distributions = np.stack(joint_distributions)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfddd9a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Grid creation\n",
    "grid = create_regular_grid(n_bins_age=20, n_bins_income=20)\n",
    "\n",
    "# Projection\n",
    "projected_distributions = np.array([\n",
    "    project_distribution_on_grid(supp.reshape(-1, 2), dist.flatten(), grid)\n",
    "    for supp, dist in zip(joint_supports, joint_distributions)\n",
    "])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd20c98d",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(support_max)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc94a3e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "i = np.random.default_rng(42)\n",
    "\n",
    "plot_projected_distribution(\n",
    "    projected_distributions[i],\n",
    "    grid_shape=(20, 20),\n",
    "    support_min=support_min,\n",
    "    support_max=support_max\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e514f4a",
   "metadata": {},
   "source": [
    "# Clustering"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20b904e3",
   "metadata": {},
   "source": [
    "## Introduction to Wasserstein K-means"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6079ec7",
   "metadata": {},
   "source": [
    "### Distance in the Wasserstein space\n",
    "\n",
    "We will now try to cluster our IRIS based on the joint-distributions we computed earlier.\n",
    "To apply a K-means algorithm to this data, we must first define a meaningful distance between observations.\n",
    "\n",
    "We choose the Earth Mover's Distance (EMD), which corresponds to the Wasserstein distance of order 1. It arises from the Kantorovich optimal transport problem:\n",
    "\n",
    "$$\n",
    "\\min_{\\pi\\in\\mathbb R_+^{n\\times m}}\n",
    "\\sum_{i=1}^n\\sum_{j=1}^m \\pi_{ij}\\,M_{ij}\n",
    "\\quad\\text{such that}\\quad \n",
    "\\sum_j \\pi_{ij} = a_i,\\quad\n",
    "\\sum_i \\pi_{ij} = b_j.\n",
    "$$\n",
    "\n",
    "Here:\n",
    "\n",
    "* The constraint $\\sum_j \\pi_{ij}=a_i$ ensures that the **first marginal** of $\\pi$ is $a$.\n",
    "* The constraint $\\sum_i \\pi_{ij}=b_j$ ensures that the **second marginal** of $\\pi$ is $b$.\n",
    "* The result of the optimization, called the **cost**, represents the **minimal total cost** of transporting mass from $a$ to $b$ under cost matrix $M$.\n",
    "\n",
    "This distance is easily computable using the [POT library](https://pythonot.github.io/) (Python Optimal Transport), via the function:\n",
    "\n",
    "```python\n",
    "cost = ot.emd2(a, b, M)\n",
    "```\n",
    "\n",
    "Where:\n",
    "\n",
    "* **`a`** is a probability vector of size $n$: $a_i \\geq 0$, $\\sum_i a_i = 1$. It represents the **first distribution**, supported on $n$ points.\n",
    "* **`b`** is a probability vector of size $m$: $b_j \\geq 0$, $\\sum_j b_j = 1$. It represents the **second distribution**, supported on $m$ points.\n",
    "* **`M`** is a cost matrix of shape $n \\times m$, where each element $M_{ij}$ represents the **cost of transporting one unit of mass** from location $i$ to location $j$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfe7c4ff",
   "metadata": {},
   "source": [
    "### Quick addition for the cost matrix\n",
    "\n",
    "All of our joint_distributions share a common discrete structure (7 age groups × 9 income deciles), because we index revenue not by its actual value but by its ordinal decile position. Concretely, each “bin” corresponds to a pair $(\\text{age\\_idx}, \\text{decile\\_idx})$, forming a fixed grid.\n",
    "\n",
    "Because this grid is identical for every IRIS, we can use a single cost matrix for all comparisons:\n",
    "\n",
    "```python\n",
    "coords = [(i, j) for i in range(7) for j in range(9)]\n",
    "cost_matrix = ot.dist(coords, coords)\n",
    "```\n",
    "\n",
    "This matrix defines the ground cost as the Euclidean distance between bins in the integer index space. This shared metric lets us compute meaningful Wasserstein distances across different IRIS efficiently, without needing to recompute it for each pair. As noted in optimal transport theory, the cost matrix is independent of the actual distributions because it only depends on how we define the “bins” themselves."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4518284",
   "metadata": {},
   "source": [
    "## Clustering based on barycenters (B-WKM)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f3d043d",
   "metadata": {},
   "source": [
    "This **centroid-based Wasserstein K-means** extends the classical Lloyd’s K-means to the space of probability measures by replacing Euclidean centroids and norms with **Wasserstein barycenters** and $W_2^2$ distances. As outlined by Domazakis et al. (2019) and formalized by Zhuang et al. (2022), the algorithm iteratively alternates between assignment and update (see below)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c7af18b",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = joint_distributions[:1000]\n",
    "n_clusters = 2\n",
    "random_state = 42\n",
    "reg=1e-1\n",
    "\n",
    "rng = np.random.default_rng(random_state)\n",
    "n_samples, n_bins = data.shape\n",
    "\n",
    "cost_matrix = ot.dist(grid, grid)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78c75362",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assignment step\n",
    "distances = np.array([\n",
    "    [ot.emd2(data[i], barycenters[k], cost_matrix) for k in range(n_clusters)]\n",
    "    for i in range(n_samples)\n",
    "])\n",
    "new_assignments = distances.argmin(axis=1)\n",
    "\n",
    "\n",
    "# Update step\n",
    "new_barycenters = []\n",
    "for k in range(n_clusters):\n",
    "    indices_k = np.where(assignments == k)[0]\n",
    "    if len(indices_k) == 0:\n",
    "        new_barycenters.append(data[rng.integers(0, n_samples)])\n",
    "    else:\n",
    "        cluster_hists = data[indices_k].T\n",
    "        bary = ot.bregman.barycenter(cluster_hists, cost_matrix, reg)\n",
    "        new_barycenters.append(bary)\n",
    "\n",
    "barycenters = new_barycenters\n",
    "\n",
    "return assignments, barycenters"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afa77f6c",
   "metadata": {},
   "source": [
    "* **Assignment step**: at each iteration, assign each distribution $\\mu_i$ to the cluster whose centroid (a Wasserstein barycenter $\\nu_k$) minimizes $W_2(\\mu_i, \\nu_k)$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1524ee0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalize the input distributions\n",
    "data = data / data.sum(axis=1, keepdims=True)\n",
    "\n",
    "# Initialisation of the barycenters\n",
    "indices = rng.choice(n_samples, size=n_clusters, replace=False)\n",
    "barycenters = data[indices].copy()\n",
    "\n",
    "assignments = np.full(n_samples, -1)\n",
    "\n",
    "for i in tqdm(range(n_samples), desc=\"Assigning to barycenters\"):\n",
    "    distances = np.zeros(n_clusters, dtype=float)\n",
    "    for j in range(n_clusters):\n",
    "        # Compute cost matrix\n",
    "        cost_matrix = ot.dist(supports[i], supports[j])\n",
    "        distances[j] = ot.emd2(data_flat[i], bary_flat[j], cost_matrix)\n",
    "    assignments[i] = np.argmin(distances)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25ebea37",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalize the input distributions\n",
    "data = data / data.sum(axis=1, keepdims=True)\n",
    "\n",
    "# Initialisation of the barycenters\n",
    "indices = rng.choice(n_samples, size=n_clusters, replace=False)\n",
    "barycenters = data[indices].copy()\n",
    "\n",
    "assignments = np.full(n_samples, -1)\n",
    "\n",
    "# Assignment step\n",
    "distances = np.array([\n",
    "    [ot.emd2(data[i], barycenters[k], cost_matrix) for k in range(n_clusters)]\n",
    "    for i in range(n_samples)\n",
    "])\n",
    "assignments = distances.argmin(axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a23592d9",
   "metadata": {},
   "source": [
    "* **Centroid update step**: recompute each cluster’s centroid as the Wasserstein barycenter of all $\\mu_i$ in that cluster—i.e., solve \n",
    "\n",
    "$$\n",
    "\\nu_k = \\arg \\min_{\\nu} \\frac 1 {|G_k|} \\sum_{i \\in G_k} W_2^2(\\mu_i,\\nu)\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00a878ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "bary_flat = []\n",
    "bary_supports = []\n",
    "\n",
    "for k in tqdm(range(n_clusters), desc=\"Recomputing barycenters\"):\n",
    "    # Indices des distributions du cluster k\n",
    "    cluster_indices = np.where(assignments == k)[0]\n",
    "\n",
    "    # If the cluster is empty, reinitialize with a random sample\n",
    "    if len(cluster_indices) == 0:\n",
    "        idx = rng.integers(0, data.shape[0])\n",
    "        bary_flat.append(data_flat[idx])\n",
    "        bary_supports.append(supports[idx])\n",
    "        continue\n",
    "\n",
    "    # Récupérer les histogrammes et les supports associés\n",
    "    cluster_hists = [data_flat[i] for i in cluster_indices]\n",
    "    cluster_supports = [supports[i] for i in cluster_indices]\n",
    "\n",
    "    # Construire le support commun (union des supports)\n",
    "    combined_support = np.unique(np.vstack(cluster_supports), axis=0)\n",
    "\n",
    "    # Reprojeter chaque histogramme sur le support commun\n",
    "    projected_hists = []\n",
    "    for hist, supp in zip(cluster_hists, cluster_supports):\n",
    "        proj = np.zeros(len(combined_support))\n",
    "        for i, atom in enumerate(supp):\n",
    "            # Trouver l’indice correspondant dans le support commun\n",
    "            idx = np.where((combined_support == atom).all(axis=1))[0][0]\n",
    "            proj[idx] = hist[i]\n",
    "        projected_hists.append(proj)\n",
    "\n",
    "    projected_hists = np.array(projected_hists).T  # (n_bins, n_distributions)\n",
    "\n",
    "    # Recalculer la cost matrix sur le support commun\n",
    "    cost_matrix = ot.dist(combined_support, combined_support)\n",
    "\n",
    "    # Calcul du barycentre régularisé\n",
    "    bary = ot.bregman.barycenter(projected_hists, cost_matrix, reg)\n",
    "\n",
    "    # Stocker le barycentre et son support\n",
    "    bary_flat.append(bary)\n",
    "    bary_supports.append(combined_support)\n",
    "\n",
    "# Reshape barycenters\n",
    "bary_hists = [b.reshape(shape) for b in bary_flat]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "996a0f1e",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "While intuitive, this method can suffer from irregularity and instability in barycenters due to the non‑Euclidean curvature of Wasserstein space, which may lead to poor cluster representation and convergence issues."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d9f069f",
   "metadata": {},
   "outputs": [],
   "source": [
    "assignments, barycenters = bary_WKMeans(\n",
    "    data=projected_distributions,\n",
    "    grid=grid,\n",
    "    n_clusters=3,\n",
    "    n_iter=10,\n",
    "    reg=1e-1,\n",
    "    random_state=None\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "848b912f",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, b in enumerate(barycenters):\n",
    "    print(f\"--- Barycentre {i} ---\")\n",
    "    print(f\"Shape: {b.shape}\")\n",
    "    print(f\"Min: {b.min():.2e}\")\n",
    "    print(f\"Max: {b.max():.2e}\")\n",
    "    print(f\"Sum: {b.sum():.6f}\")\n",
    "    print(f\"Contains NaN: {np.isnan(b).any()}\")\n",
    "    print(f\"Zero elements: {np.sum(b == 0)} / {b.size}\")\n",
    "    print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa5c8f8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_bins_age = 20\n",
    "n_bins_income = 20\n",
    "extent = [grid[:, 1].min(), grid[:, 1].max(), grid[:, 0].min(), grid[:, 0].max()]\n",
    "\n",
    "for i, barycenter in enumerate(barycenters):\n",
    "    barycenter = barycenter.reshape(n_bins_age, n_bins_income)\n",
    "    \n",
    "    plt.figure(figsize=(6, 5))\n",
    "    plt.imshow(barycenter, origin='lower', cmap='viridis', extent=extent, aspect='auto')\n",
    "    plt.colorbar(label=\"Probability\")\n",
    "    plt.title(f\"Barycentre {i+1}\")\n",
    "    plt.xlabel(\"Décile de revenu\")\n",
    "    plt.ylabel(\"Classe d'âge\")\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "204e35d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "k_values = range(1, 11)\n",
    "cost_matrix = ot.dist(grid, grid)\n",
    "\n",
    "inertias = []\n",
    "for k_test in tqdm(k_values):\n",
    "    assignments, barycenters = bary_WKMeans(\n",
    "        data=projected_distributions,\n",
    "        grid=grid,\n",
    "        n_clusters=k_test,\n",
    "        n_iter=100,\n",
    "        reg=1e-1,\n",
    "        random_state=None)\n",
    "\n",
    "    inertia = sum(\n",
    "        ot.emd2(projected_distributions[i], barycenters[assignments[i]], cost_matrix)\n",
    "        for i in range(len(projected_distributions))\n",
    "    )\n",
    "    inertias.append(inertia)\n",
    "\n",
    "plt.figure(figsize=(8, 5))\n",
    "plt.plot(k_values, inertias, marker='o')\n",
    "plt.title(\"Elbow Method for Optimal k\")\n",
    "plt.xlabel(\"Number of clusters k\")\n",
    "plt.ylabel(\"Total within-cluster Wasserstein distance\")\n",
    "plt.grid(True)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f274c8eb",
   "metadata": {},
   "source": [
    "## Clustering based on pairwise distances (D-WKM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75945a0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "pairwise_france = computeDistanceMatrix(\n",
    "    data = projected_distributions,\n",
    "    grid = grid,\n",
    "    save=True,\n",
    "    filepath='../data/Dis_mat_france.txt'\n",
    ")\n",
    "\n",
    "# pairwise_france = np.loadtxt(\"../data/Dis_mat_france.txt\", dtype=float)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c8fd5ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "assignments_france = dist_WKMeans(\n",
    "    data = projected_distributions[:10],\n",
    "    grid = grid,\n",
    "    dist_matrix=pairwise_france,\n",
    "    n_clusters=3,\n",
    "    n_iter=40,\n",
    "    random_state=None\n",
    ")\n",
    "\n",
    "np.unique(assignments_france)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de54f4d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Only Paris here\n",
    "iris_paris = [iris for iris in common_iris if iris.startswith(\"75\")]\n",
    "distributions_paris = np.array([projected_distributions[common_iris.index(iris)] for iris in iris_paris])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af962d1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "pairwise_paris = computeDistanceMatrix(\n",
    "    data = projected_distributions,\n",
    "    grid = grid,\n",
    "    save=True,\n",
    "    filepath='../data/Dis_mat_paris.txt'\n",
    ")\n",
    "\n",
    "# pairwise_france = np.loadtxt(\"../data/Dis_mat_paris.txt\", dtype=float)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d041fabf",
   "metadata": {},
   "outputs": [],
   "source": [
    "assignments_paris = dist_WKMeans(\n",
    "    data = distributions_paris,\n",
    "    grid = grid,\n",
    "    dist_matrix=pairwise_paris,\n",
    "    n_clusters=3,\n",
    "    n_iter=40,\n",
    "    random_state=None\n",
    ")\n",
    "\n",
    "np.unique(assignments_paris)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53002e31",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pour 1000 observations (3 clusters, 5it)\n",
    "# clustering_methods : 2m35.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa0f50aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "k_values = range(1, 11)\n",
    "\n",
    "inertias = []\n",
    "for k_test in tqdm(k_values):\n",
    "    assignments, barycenters = wbarycenter_clustering(\n",
    "    data=joint_distributions,  # (n_iris, 7, 9)\n",
    "    n_clusters=k_test,\n",
    "    n_iter=5,\n",
    "    reg=0.1,\n",
    "    random_state=42\n",
    ")\n",
    "    inertia = 0\n",
    "    for i in range(n_distributions):\n",
    "        bary = barycenters[assignments[i]]\n",
    "        inertia += ot.wasserstein_1d(support, support, distributions[i], bary)\n",
    "    inertias.append(inertia)\n",
    "\n",
    "plt.figure(figsize=(8, 5))\n",
    "plt.plot(k_values, inertias, marker='o')\n",
    "plt.title(\"Elbow Method for Optimal k\")\n",
    "plt.xlabel(\"Number of clusters k\")\n",
    "plt.ylabel(\"Total within-cluster Wasserstein distance\")\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4058e999",
   "metadata": {},
   "source": [
    "# Mapping"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb2c9c1c",
   "metadata": {},
   "source": [
    "## France"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abed8d5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "gdf_france_raw = gpd.read_file(\"../data/contours-iris.gpkg\")\n",
    "\n",
    "gdf_france = gdf_france_raw[gdf_france_raw[\"code_iris\"].isin(common_iris)].copy()\n",
    "gdf_france[\"code_iris\"] = gdf_france[\"code_iris\"].astype(str)\n",
    "\n",
    "\n",
    "iris_cluster_df = pd.DataFrame({\n",
    "    \"code_iris\": common_iris,\n",
    "    \"cluster\": assignments_france\n",
    "})\n",
    "\n",
    "gdf_france = gdf_france.merge(iris_cluster_df, on=\"code_iris\", how=\"left\")\n",
    "\n",
    "gdf_france['cluster'] = gdf_france['cluster'].astype('category')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b354ae55",
   "metadata": {},
   "outputs": [],
   "source": [
    "gdf_france.plot(\n",
    "    column='cluster',\n",
    "    cmap='Accent',\n",
    "    legend=True,\n",
    "    legend_kwds={'title': 'Cluster'},  # Ajoute un titre clair à la légende\n",
    "    missing_kwds={'color': 'lightgrey', 'label': 'No data'}\n",
    ")\n",
    "plt.title(\"Wasserstein Clusters - France\")\n",
    "plt.axis('off')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d5329af",
   "metadata": {},
   "source": [
    "## Paris"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8004109",
   "metadata": {},
   "outputs": [],
   "source": [
    "iris_paris = [iris for iris in common_iris if iris.startswith(\"75\")]\n",
    "assignments_proj = np.array([assignments_france[common_iris.index(iris)] for iris in iris_paris])\n",
    "\n",
    "gdf_paris = gdf_france_raw[gdf_france_raw[\"code_iris\"].isin(iris_paris)].copy()\n",
    "\n",
    "iris_cluster_df = pd.DataFrame({\n",
    "    \"code_iris\": iris_paris,\n",
    "    \"cluster_proj\": assignments_proj,\n",
    "    \"cluster_paris\": assignments_paris\n",
    "})\n",
    "\n",
    "gdf_paris = gdf_paris.merge(iris_cluster_df, on=\"code_iris\", how=\"left\")\n",
    "\n",
    "gdf_paris['cluster_proj'] = gdf_paris['cluster_proj'].astype('category')\n",
    "gdf_paris['cluster_paris'] = gdf_paris['cluster_paris'].astype('category')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1428e8b3",
   "metadata": {},
   "source": [
    "### France clustering projected on Paris"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad957ba5",
   "metadata": {},
   "outputs": [],
   "source": [
    "gdf_paris.plot(\n",
    "    column='cluster_proj',\n",
    "    cmap='Accent',\n",
    "    legend=True,\n",
    "    legend_kwds={'title': 'Cluster'},  # Ajoute un titre clair à la légende\n",
    "    edgecolor='black',\n",
    "    linewidth=0.1,\n",
    "    missing_kwds={'color': 'lightgrey', 'label': 'No data'}\n",
    ")\n",
    "plt.title(\"Wasserstein Clusters - Paris\")\n",
    "plt.axis('off')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4c2d5c8",
   "metadata": {},
   "source": [
    "### Paris Clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f53088be",
   "metadata": {},
   "outputs": [],
   "source": [
    "gdf_paris.plot(\n",
    "    column='cluster_paris',\n",
    "    cmap='Accent',\n",
    "    legend=True,\n",
    "    legend_kwds={'title': 'Cluster'},  # Ajoute un titre clair à la légende\n",
    "    edgecolor='black',\n",
    "    linewidth=0.1,\n",
    "    missing_kwds={'color': 'lightgrey', 'label': 'No data'}\n",
    ")\n",
    "plt.title(\"Wasserstein Clusters - Paris\")\n",
    "plt.axis('off')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5509055",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_clusters = 3\n",
    "fig, axes = plt.subplots(2, n_clusters, figsize=(5*n_clusters, 8))\n",
    "\n",
    "# France clustering projected on Paris\n",
    "for k in range(n_clusters):\n",
    "    ax = axes[0, k]\n",
    "    idx = rng.choice(np.where(assignments_proj == k)[0])\n",
    "\n",
    "    dist = joint_distributions_paris[idx]\n",
    "    supp = joint_supports_paris[idx]\n",
    "    age_classes = supp[:, 0]\n",
    "    income_deciles = supp[:, 1]\n",
    "\n",
    "    scatter = ax.scatter(\n",
    "        income_deciles, age_classes,\n",
    "        c=dist,\n",
    "        cmap='viridis',\n",
    "        s=300 * dist,\n",
    "        edgecolor='k'\n",
    "    )\n",
    "    ax.set_title(f\"France Clustering - Cluster {k}\")\n",
    "    ax.set_xlabel(\"Income\")\n",
    "    ax.set_ylabel(\"Age\")\n",
    "    ax.set_xlim(0, 1)\n",
    "    ax.set_ylim(0, 1)\n",
    "    ax.grid(True)\n",
    "\n",
    "# Paris clustering\n",
    "for k in range(n_clusters):\n",
    "    ax = axes[1, k]\n",
    "    idx = rng.choice(np.where(assignments_paris == k)[0])\n",
    "\n",
    "    dist = joint_distributions_paris[idx]\n",
    "    supp = joint_supports_paris[idx]\n",
    "    age_classes = supp[:, 0]\n",
    "    income_deciles = supp[:, 1]\n",
    "\n",
    "    scatter = ax.scatter(\n",
    "        income_deciles, age_classes,\n",
    "        c=dist,\n",
    "        cmap='viridis',\n",
    "        s=300 * dist,\n",
    "        edgecolor='k'\n",
    "    )\n",
    "    ax.set_title(f\"Paris Clustering - Cluster {k}\")\n",
    "    ax.set_xlabel(\"Income\")\n",
    "    ax.set_ylabel(\"Age\")\n",
    "    ax.set_xlim(0, 1)\n",
    "    ax.set_ylim(0, 1)\n",
    "    ax.grid(True)\n",
    "\n",
    "fig.tight_layout()\n",
    "cbar = fig.colorbar(scatter, ax=axes.ravel().tolist(), shrink=0.95)\n",
    "cbar.set_label(\"Probabilité\")\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
